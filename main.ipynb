{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Soups\n",
    "\n",
    "In this assignment, you will be implementing [Model Soups](https://arxiv.org/pdf/2203.05482.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jax-resnet\n",
      "  Downloading jax_resnet-0.0.4-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: jax in /Users/edanbash/opt/anaconda3/lib/python3.9/site-packages (from jax-resnet) (0.3.25)\n",
      "Requirement already satisfied: jaxlib in /Users/edanbash/opt/anaconda3/lib/python3.9/site-packages (from jax-resnet) (0.3.25)\n",
      "Collecting flax\n",
      "  Downloading flax-0.6.2-py3-none-any.whl (189 kB)\n",
      "\u001b[K     |████████████████████████████████| 189 kB 9.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.12 in /Users/edanbash/opt/anaconda3/lib/python3.9/site-packages (from flax->jax-resnet) (1.21.5)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in /Users/edanbash/opt/anaconda3/lib/python3.9/site-packages (from flax->jax-resnet) (4.1.1)\n",
      "Requirement already satisfied: matplotlib in /Users/edanbash/opt/anaconda3/lib/python3.9/site-packages (from flax->jax-resnet) (3.5.1)\n",
      "Requirement already satisfied: msgpack in /Users/edanbash/opt/anaconda3/lib/python3.9/site-packages (from flax->jax-resnet) (1.0.2)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /Users/edanbash/opt/anaconda3/lib/python3.9/site-packages (from flax->jax-resnet) (6.0)\n",
      "Collecting optax\n",
      "  Downloading optax-0.1.4-py3-none-any.whl (154 kB)\n",
      "\u001b[K     |████████████████████████████████| 154 kB 30.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorstore\n",
      "  Downloading tensorstore-0.1.28-cp39-cp39-macosx_10_14_x86_64.whl (9.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.2 MB 20.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting rich>=11.1\n",
      "  Downloading rich-12.6.0-py3-none-any.whl (237 kB)\n",
      "\u001b[K     |████████████████████████████████| 237 kB 25.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.5 in /Users/edanbash/opt/anaconda3/lib/python3.9/site-packages (from jax->jax-resnet) (1.7.3)\n",
      "Requirement already satisfied: opt-einsum in /Users/edanbash/opt/anaconda3/lib/python3.9/site-packages (from jax->jax-resnet) (3.3.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /Users/edanbash/opt/anaconda3/lib/python3.9/site-packages (from rich>=11.1->flax->jax-resnet) (2.11.2)\n",
      "Collecting commonmark<0.10.0,>=0.9.0\n",
      "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
      "\u001b[K     |████████████████████████████████| 51 kB 20.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: kiwisolver>=1.0.1 in /Users/edanbash/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->flax->jax-resnet) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/edanbash/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->flax->jax-resnet) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/edanbash/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->flax->jax-resnet) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/edanbash/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->flax->jax-resnet) (3.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/edanbash/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->flax->jax-resnet) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/edanbash/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->flax->jax-resnet) (4.25.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/edanbash/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->flax->jax-resnet) (9.0.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/edanbash/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib->flax->jax-resnet) (1.16.0)\n",
      "Collecting chex>=0.1.5\n",
      "  Downloading chex-0.1.5-py3-none-any.whl (85 kB)\n",
      "\u001b[K     |████████████████████████████████| 85 kB 7.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py>=0.7.1\n",
      "  Downloading absl_py-1.3.0-py3-none-any.whl (124 kB)\n",
      "\u001b[K     |████████████████████████████████| 124 kB 11.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dm-tree>=0.1.5\n",
      "  Downloading dm_tree-0.1.7-cp39-cp39-macosx_10_9_x86_64.whl (109 kB)\n",
      "\u001b[K     |████████████████████████████████| 109 kB 40.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: toolz>=0.9.0 in /Users/edanbash/opt/anaconda3/lib/python3.9/site-packages (from chex>=0.1.5->optax->flax->jax-resnet) (0.11.2)\n",
      "Installing collected packages: dm-tree, absl-py, commonmark, chex, tensorstore, rich, optax, flax, jax-resnet\n",
      "Successfully installed absl-py-1.3.0 chex-0.1.5 commonmark-0.9.1 dm-tree-0.1.7 flax-0.6.2 jax-resnet-0.0.4 optax-0.1.4 rich-12.6.0 tensorstore-0.1.28\n"
     ]
    }
   ],
   "source": [
    "!pip install jax-resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from typing import Callable\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "#import tensorflow.keras as keras\n",
    "#import tensorflow as tf\n",
    "#import tensorflow_datasets as tfds \n",
    "\n",
    "import jax\n",
    "import optax\n",
    "import flax\n",
    "import jax.numpy as jnp\n",
    "from jax import jit\n",
    "from jax import lax\n",
    "from jax_resnet import pretrained_resnet, slice_variables, Sequential\n",
    "from flax.jax_utils import replicate, unreplicate\n",
    "from flax.training import train_state\n",
    "from flax import linen as nn\n",
    "from flax.core import FrozenDict,frozen_dict\n",
    "from flax.training.common_utils import shard\n",
    "\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Config = {\n",
    "    'NUM_LABELS': 10,\n",
    "    'N_SPLITS': 5,\n",
    "    'BATCH_SIZE': 32,\n",
    "    'N_EPOCHS': 10,\n",
    "    'LR': 0.001,\n",
    "    'WIDTH': 32,\n",
    "    'HEIGHT': 32,\n",
    "    'IMAGE_SIZE': 128,\n",
    "    'WEIGHT_DECAY': 1e-5,\n",
    "    'FREEZE_BACKBONE': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA PREPROCESSING\n",
    "\n",
    "def transform_images(row, size):\n",
    "    '''\n",
    "    Resize image \n",
    "    INPUT row , size\n",
    "    RETURNS resized image and its label\n",
    "    '''\n",
    "    x_train = tf.image.resize(row['image'], (size, size))\n",
    "    return x_train, row['label']\n",
    "\n",
    "def load_datasets():\n",
    "    '''\n",
    "    load and transform dataset from tfds\n",
    "    RETURNS train and test dataset\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Construct a tf.data.Dataset\n",
    "    train_ds,test_ds = tfds.load('cifar10', split=['train','test'], shuffle_files=True)\n",
    "\n",
    "    train_ds = train_ds.map(lambda row:transform_images(row,Config[\"IMAGE_SIZE\"]))\n",
    "    test_ds = test_ds.map(lambda row:transform_images(row,Config[\"IMAGE_SIZE\"]))\n",
    "    \n",
    "    # Build your input pipeline\n",
    "    train_dataset = train_ds.batch(Config[\"BATCH_SIZE\"]).prefetch(tf.data.AUTOTUNE)\n",
    "    test_dataset = test_ds.batch(Config[\"BATCH_SIZE\"]).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return train_dataset,test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINING THE MODEL\n",
    "\"\"\"\n",
    "reference - https://www.kaggle.com/code/alexlwh/happywhale-flax-jax-tpu-gpu-resnet-baseline\n",
    "\"\"\"\n",
    "class MarginLayer(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, inputs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class Head(nn.Module):\n",
    "    '''head model'''\n",
    "    batch_norm_cls: partial = partial(nn.BatchNorm, momentum=0.9)\n",
    "    @nn.compact\n",
    "    def __call__(self, inputs, train: bool):\n",
    "        output_n = inputs.shape[-1]\n",
    "        x = self.batch_norm_cls(use_running_average=not train)(inputs)\n",
    "        x = nn.Dropout(rate=0.25)(x, deterministic=not train)\n",
    "        x = nn.Dense(features=output_n)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = self.batch_norm_cls(use_running_average=not train)(x)\n",
    "        x = nn.Dropout(rate=0.5)(x, deterministic=not train)\n",
    "        x = nn.Dense(features=Config[\"NUM_LABELS\"])(x)\n",
    "        return x\n",
    "\n",
    "class Model(nn.Module):\n",
    "    '''Combines backbone and head model'''\n",
    "    backbone: Sequential\n",
    "    head: Head\n",
    "        \n",
    "    def __call__(self, inputs, train: bool):\n",
    "        x = self.backbone(inputs)\n",
    "        # average pool layer\n",
    "        x = jnp.mean(x, axis=(1, 2))\n",
    "        x = self.head(x, train)\n",
    "        return x\n",
    "\n",
    "    \n",
    "def _get_backbone_and_params(model_arch: str):\n",
    "    '''\n",
    "    Get backbone and params\n",
    "    1. Loads pretrained model (resnet18)\n",
    "    2. Get model and param structure except last 2 layers\n",
    "    3. Extract the corresponding subset of the variables dict\n",
    "    INPUT : model_arch\n",
    "    RETURNS backbone , backbone_params\n",
    "    '''\n",
    "    if model_arch == 'resnet18':\n",
    "        resnet_tmpl, params = pretrained_resnet(18)\n",
    "        model = resnet_tmpl()\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # get model & param structure for backbone\n",
    "    start, end = 0, len(model.layers) - 2\n",
    "    backbone = Sequential(model.layers[start:end])\n",
    "    backbone_params = slice_variables(params, start, end)\n",
    "    return backbone, backbone_params\n",
    "\n",
    "\n",
    "def get_model_and_variables(model_arch: str, head_init_key: int):\n",
    "    '''\n",
    "    Get model and variables \n",
    "    1. Initialise inputs(shape=(1,image_size,image_size,3))\n",
    "    2. Get backbone and params\n",
    "    3. Apply backbone model and get outputs\n",
    "    4. Initialise head\n",
    "    5. Create final model using backbone and head\n",
    "    6. Combine params from backbone and head\n",
    "    \n",
    "    INPUT model_arch, head_init_key\n",
    "    RETURNS  model, variables \n",
    "    '''\n",
    "    \n",
    "    #backbone\n",
    "    inputs = jnp.ones((1, Config['IMAGE_SIZE'],Config['IMAGE_SIZE'], 3), jnp.float32)\n",
    "    backbone, backbone_params = _get_backbone_and_params(model_arch)\n",
    "    key = jax.random.PRNGKey(head_init_key)\n",
    "    backbone_output = backbone.apply(backbone_params, inputs, mutable=False)\n",
    "    \n",
    "    #head\n",
    "    head_inputs = jnp.ones((1, backbone_output.shape[-1]), jnp.float32)\n",
    "    head = Head()\n",
    "    head_params = head.init(key, head_inputs, train=False)\n",
    "    \n",
    "    #final model\n",
    "    model = Model(backbone, head)\n",
    "    variables = FrozenDict({\n",
    "        'params': {\n",
    "            'backbone': backbone_params['params'],\n",
    "            'head': head_params['params']\n",
    "        },\n",
    "        'batch_stats': {\n",
    "            'backbone': backbone_params['batch_stats'],\n",
    "            'head': head_params['batch_stats']\n",
    "        }\n",
    "    })\n",
    "    return model, variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/pytorch/vision/zipball/v0.6.0\" to /Users/edanbash/.cache/torch/hub/v0.6.0.zip\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /Users/edanbash/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8803c8d7aeeb43de84e90303a37acd66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Loading in pre-trained ResNet18 Model\n",
    "\n",
    "model, variables = get_model_and_variables('resnet18', 0)\n",
    "inputs = jnp.ones((1,Config['IMAGE_SIZE'], Config['IMAGE_SIZE'],3), jnp.float32)\n",
    "key = jax.random.PRNGKey(0)\n",
    "o = model.apply(variables, inputs, train=False, mutable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background \n",
    "Pre-training <br>\n",
    "Hyperparameter Search <br>\n",
    "Ensemble Learning <br>\n",
    "Greedy Algorithms <br> \n",
    "Vision Transformers <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A. Fine Tuning a single ViT model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B. Fine-Tuning various models (and pick the best one)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C. Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part D. Uniform Soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part E. Greedy Soup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "a42fd9c54dfef7db26bc855dec81a1743f6591eb1f4b14dcb57cc543b956a047"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
